# -*- coding: utf-8 -*-
"""elbom

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tDdc8A1oFYUqzISQOSbk_fy6TB9d7hwv
"""



#instalando dependencia
#pip install nltk

# Data Structures
import numpy  as np
import pandas as pd

import json

# Corpus Processing
import re
import nltk.corpus
import nltk
from unidecode                        import unidecode
nltk.download('stopwords')
import nltk
nltk.download('punkt')

from nltk.tokenize                    import word_tokenize
from nltk                             import SnowballStemmer
from sklearn.feature_extraction.text  import TfidfVectorizer
from sklearn.feature_extraction       import DictVectorizer
from sklearn.preprocessing            import normalize

# K-Means
from sklearn import cluster

# Visualization and Analysis
import matplotlib.pyplot  as plt
import matplotlib.cm      as cm
import seaborn            as sns
from sklearn.metrics                  import silhouette_samples, silhouette_score
from wordcloud                        import WordCloud

# Map Viz
import folium
#import branca.colormap as cm
from branca.element import Figure

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline 
from sklearn.cluster import KMeans
from sklearn import datasets
iris = datasets.load_iris()

#uri = "https://raw.githubusercontent.com/RenatoGalindo/K-means-acur-cia/master/pergunta.csv"
uri = "/content/3. Amostra_CampoPerguntaAposTratamento_New.csv"

data = pd.read_csv(uri ,error_bad_lines=False )
data.columns = map(str.lower, data.columns)
data.head(110 )

#pegando a clouna pergunta
corpus = data['pergunta'].tolist()
#imprindo as colunas
print(corpus)

# removes a list of words (ie. stopwords) from a tokenized list.
def removeWords(listOfTokens, listOfWords):
    return [token for token in listOfTokens if token not in listOfWords]

# applies stemming to a list of tokenized words
def applyStemming(listOfTokens, stemmer):
    return [stemmer.stem(token) for token in listOfTokens]

# removes any words composed of less than 2 or more than 21 letters
def twoLetters(listOfTokens):
    twoLetterWord = []
    for token in listOfTokens:
        if len(token) <= 2 or len(token) >= 21:
            twoLetterWord.append(token)
    return twoLetterWord

def processCorpus(corpus, language):   
    stopwords = nltk.corpus.stopwords.words(language)
    param_stemmer = SnowballStemmer(language)


    
    for document in corpus:

        index = corpus.index(document)
        
        corpus[index] = corpus[index].replace(u'\ufffd', '8')   # Replaces the ASCII 'ï¿½' symbol with '8'
        corpus[index] = corpus[index].replace(',', '')          # Removes commas
        corpus[index] = corpus[index].rstrip('\n')              # Removes line breaks
        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase
        
        corpus[index] = re.sub('\W_',' ', corpus[index])        # removes specials characters and leaves only words
        corpus[index] = re.sub("\S*\d\S*"," ", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.
        corpus[index] = re.sub("\S*@\S*\s?"," ", corpus[index]) # removes emails and mentions (words with @)
        corpus[index] = re.sub(r'http\S+', '', corpus[index])   # removes URLs with http
        corpus[index] = re.sub(r'www\S+', '', corpus[index])    # removes URLs with www
       
       
        listOfTokens = word_tokenize(corpus[index])
      
 
        twoLetterWord = twoLetters(listOfTokens)
        listOfTokens = removeWords(listOfTokens, stopwords)
      
       
        
        
        listOfTokens = removeWords(listOfTokens, twoLetterWord)

        
        
        corpus[index]   = " ".join(listOfTokens)
     
        
        corpus[index] = unidecode(corpus[index])
       

        

    return corpus

language = 'portuguese'

corpus = processCorpus(corpus, language)

print(corpus)

vectorizer = TfidfVectorizer(sublinear_tf=True)



X = vectorizer.fit_transform(corpus)



 


distortions = []
K = range(1,500)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(X.toarray())
    distortions.append(kmeanModel.inertia_)

plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')

plt.show()